{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from f110_gym.envs.base_classes import Integrator\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "import sys\n",
    "from f110_gym.envs.f110_env import F110Env\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='SARSA_no_speed_binary_logs.log',\n",
    "    filemode='a'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward:\n",
    "    def __init__(self, min_speed=0.8, max_speed=2,num_speeds=5 ,map_centers=None,track_width = 2.2,logger = None):\n",
    "        # Keep existing parameters\n",
    "        self.min_speed = min_speed\n",
    "        self.max_speed = max_speed\n",
    "        self.num_speeds = num_speeds\n",
    "        self.speeds = np.linspace(min_speed, max_speed, num_speeds)\n",
    "        self.mean_speed = np.mean(self.speeds)\n",
    "        self.std_speed = np.std(self.speeds)\n",
    "        self.set_parameters(map_centers,track_width)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.epsilon = 1e-5\n",
    "        self.distance_travelled = 0\n",
    "        self.milestone = 0\n",
    "        self.centerline_scale = 5\n",
    "        self.progress_scale = 1.0\n",
    "        self.logger = logger\n",
    "\n",
    "    def set_parameters(self,map_centers, track_width):\n",
    "        '''\n",
    "        Helper function to set the parameters of the reward function externally from the class instance\n",
    "        Args:\n",
    "            map_centers (np.ndarray): Array of map centers.\n",
    "            track_width (float): Width of the track.\n",
    "        '''\n",
    "        self.map_centers = map_centers\n",
    "        # Initial point and center that determines the position at the start of episode\n",
    "        self.initial_point = np.array([[0, 0]])\n",
    "        self.initial_center_idx , _ = self.__calculate_distance_from_center(self.map_centers,self.initial_point)\n",
    "        self.initial_center = self.map_centers[self.initial_center_idx]\n",
    "\n",
    "        # Race Track parameters\n",
    "        self.distance_between_centers = np.hstack([[0.],np.linalg.norm(self.map_centers[:-1,:]- self.map_centers[1:,:],axis=1)])\n",
    "        self.total_track_length = np.sum(self.distance_between_centers)\n",
    "        self.track_width = track_width\n",
    "    \n",
    "    def __calculate_distance_from_center(self, centers,curr):\n",
    "        '''\n",
    "        Helper function to calculate the distance from the all centers of the track to the current position\n",
    "        Args:\n",
    "            centers (np.ndarray): Array of map centers.\n",
    "            curr (np.ndarray): Current position of the agent.\n",
    "        Returns:\n",
    "            idx (int): Index of the closest center.\n",
    "            distance (float): Distance to the closest center.\n",
    "        '''\n",
    "        distances = np.linalg.norm(centers - curr, axis=1)\n",
    "        idx = np.argmin(distances)\n",
    "        return idx, distances[idx]\n",
    "    \n",
    "    def reset(self, pos):\n",
    "        \"\"\"\n",
    "        Reset the reward function state, supporting arbitrary starting position and heading.\n",
    "        \n",
    "        Args:\n",
    "            pos (np.ndarray): Starting position of the agent.\n",
    "            heading (float, optional): Starting heading of the agent in radians.\n",
    "        \"\"\"\n",
    "        self.distance_travelled = 0\n",
    "        self.milestone = 0\n",
    "        self.initial_point = pos\n",
    "        \n",
    "        # Initialize starting center reference\n",
    "        self.initial_center_idx, _ = self.__calculate_distance_from_center(self.map_centers, self.initial_point)\n",
    "        self.initial_center = self.map_centers[self.initial_center_idx]\n",
    "    \n",
    "    def centerline_reward(self, curr_pos, next_pos, curr_center_idx, next_center_idx, curr_dist,next_dist ,threshold_angle=0.2, threshold_dist = 0.35):\n",
    "        \"\"\"\n",
    "        Calculate the centerline reward based on the distance from the centerline. \n",
    "        Angles are computred in radians.\n",
    "        Restricting the large negative reward to -1000 and positive to 100\n",
    "        \n",
    "        Args:\n",
    "            pos (np.ndarray): Current position of the agent.\n",
    "        \n",
    "        Returns:\n",
    "            float: Centerline reward.\n",
    "        \"\"\"\n",
    "        \n",
    "        if curr_center_idx == next_center_idx:\n",
    "            next_center_idx = (next_center_idx + 1) % len(self.map_centers)\n",
    "        \n",
    "        movement_vector = next_pos - curr_pos\n",
    "        movement_vector /= (np.linalg.norm(movement_vector) + self.epsilon)\n",
    "\n",
    "        centerline_vector = self.map_centers[next_center_idx] - self.map_centers[curr_center_idx]\n",
    "        centerline_vector /= (np.linalg.norm(centerline_vector) + self.epsilon)\n",
    "\n",
    "        angle = np.arctan2(centerline_vector[1], centerline_vector[0]) - np.arctan2(movement_vector[1], movement_vector[0])\n",
    "        angle = np.arctan2(np.sin(angle), np.cos(angle))  # Normalize angle to [-pi, pi]\n",
    "\n",
    "        angle = abs(angle)  # Consider only the absolute angle deviation\n",
    "\n",
    "        if angle <= threshold_angle:\n",
    "            reward = 2.0 - (angle / threshold_angle)\n",
    "        else:\n",
    "            # Exponential decay\n",
    "            reward = -4.3 * np.exp(3 * (angle - threshold_angle))\n",
    "\n",
    "        # print(f'Angle: {angle}, Angle Reward: {reward}')\n",
    "        \n",
    "        if next_dist <= threshold_dist:\n",
    "            # Reward increases as distance gets closer to 0\n",
    "            dist_reward = 2.0 * (1 - next_dist / threshold_dist)\n",
    "        else:\n",
    "            # Penalize exponentially as distance increases beyond threshold\n",
    "            dist_reward = -2.0 * np.exp(1.5 * (next_dist - threshold_dist))\n",
    "        \n",
    "        # print(f'Next Distance: {next_dist},Distance Reward: {dist_reward}')\n",
    "        # Combine rewards\n",
    "        total_reward = reward + dist_reward\n",
    "        \n",
    "        # You can also add a bonus if the agent is improving its distance to centerline\n",
    "        if next_dist < curr_dist:\n",
    "            total_reward += 1  # Small bonus for getting closer to centerline\n",
    "\n",
    "        return np.clip(total_reward,-1000,100)       \n",
    "    \n",
    "    def progress_reward(self, curr_pos, next_pos, curr_center_idx, next_center_idx):\n",
    "        \"\"\"\n",
    "        Calculate the progress reward based on the distance travelled along the track. \n",
    "        Restricting the positive reward to +100\n",
    "        \n",
    "        Args:\n",
    "            curr_pos (np.ndarray): Current position of the agent.\n",
    "            next_pos (np.ndarray): Next position of the agent.\n",
    "            curr_center_idx (int): Index of the current center.\n",
    "            next_center_idx (int): Index of the next center.\n",
    "        \n",
    "        Returns:\n",
    "            float: Progress reward.\n",
    "        \"\"\"\n",
    "        if curr_center_idx == next_center_idx:\n",
    "            next_center_idx = (next_center_idx + 1) % len(self.map_centers)\n",
    "        \n",
    "        distance = np.linalg.norm(next_pos - curr_pos)\n",
    "        self.distance_travelled += distance\n",
    "\n",
    "        reward = self.distance_travelled\n",
    "        if self.distance_travelled > 100:\n",
    "            self.distance_travelled = 0\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def calculate_reward(self,curr_pos,next_pos,speed):\n",
    "        \"\"\"\n",
    "        Calculate the total reward based on centerline and progress rewards.\n",
    "        \n",
    "        Args:\n",
    "            curr_pos (np.ndarray): Current position of the agent.\n",
    "            next_pos (np.ndarray): Next position of the agent.\n",
    "        \n",
    "        Returns:\n",
    "            float: Total reward.\n",
    "        \"\"\"\n",
    "        curr_center_idx, curr_dist = self.__calculate_distance_from_center(self.map_centers, curr_pos)\n",
    "        next_center_idx, next_dist = self.__calculate_distance_from_center(self.map_centers, next_pos)\n",
    "\n",
    "        centerline_reward = self.centerline_reward(curr_pos, next_pos, curr_center_idx, next_center_idx, curr_dist, next_dist)\n",
    "        # progress_reward = self.progress_reward(curr_pos, next_pos, curr_center_idx, next_center_idx)\n",
    "        \n",
    "        total_reward = centerline_reward #+ progress_reward\n",
    "\n",
    "        # print(f'Centerline Reward: {centerline_reward}, Speed reward: {speed_reward} \\n Total Reward: {total_reward}')  # Progress Reward: {progress_reward}, \\n Total Reward: {total_reward}\n",
    "\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexSelector:\n",
    "    def __init__(self, num_indices):\n",
    "        self.set_parameters(num_indices)\n",
    "    \n",
    "    def set_parameters(self, num_indices):\n",
    "        self.num_indices = num_indices\n",
    "        self.visited_indices = set()\n",
    "        self.probabilities = np.ones(num_indices) / num_indices\n",
    "        self.all_indices = np.arange(self.num_indices)\n",
    "    \n",
    "    def select_index(self):\n",
    "        if len(self.visited_indices) == self.num_indices:\n",
    "            # Reset the probabilities and visited indices\n",
    "            print('Visited all indices, resetting')\n",
    "            self.visited_indices = set()\n",
    "            self.probabilities = np.ones(self.num_indices) / self.num_indices\n",
    "\n",
    "        # Select an index based on the current probabilities\n",
    "        random_idx = np.random.choice(self.all_indices, p=self.probabilities)\n",
    "\n",
    "        # Update the probabilities\n",
    "        self.visited_indices.add(random_idx)\n",
    "        if len(self.visited_indices) < self.num_indices:\n",
    "            self.probabilities[random_idx] = 0\n",
    "            remaining_prob = 1 - np.sum(self.probabilities)\n",
    "            self.probabilities[self.probabilities > 0] += remaining_prob / np.sum(self.probabilities > 0)\n",
    "\n",
    "        return random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Tenth_navigation:\n",
    "\n",
    "    def __init__(self,gym_env_code='f110_gym:f110-v0', num_agents=1, map_path=['./f1tenth_racetracks/Austin/Austin_map'], map_ext='.png', sx=0., sy=0., stheta=0., map_centers_file=None, save_path=None, track_name=None, inference=None,reward_file=None,collision_file=None):\n",
    "\n",
    "        # Environment setup\n",
    "        self.path_counter = 0\n",
    "        self.sx, self.sy, self.stheta = sx, sy, stheta\n",
    "        self.save_path = save_path\n",
    "        self.track_name = track_name\n",
    "        self.num_agents = num_agents\n",
    "        self.map_path = map_path\n",
    "        self.map_ext = map_ext\n",
    "        self.map_centers_file = map_centers_file\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(f'Initializing F1Tenth_navigation with map: {self.map_path[self.path_counter]}')\n",
    "    \n",
    "        self.env = gym.make(gym_env_code, map=self.map_path[self.path_counter], map_ext=self.map_ext, num_agents=self.num_agents, timestep=0.01, integrator=Integrator.RK4)\n",
    "        self.env.add_render_callback(self.render_callback)\n",
    "       \n",
    "        file = pd.read_csv(self.map_centers_file[self.path_counter])\n",
    "        file.columns = ['x', 'y', 'w_r', 'w_l']\n",
    "        file.index = file.index.astype(int)\n",
    "        self.map_centers = file.values[:, :2]\n",
    "        self.track_headings = self.calculate_track_headings(self.map_centers)\n",
    "        self.track_width = file.loc[0,'w_r'] + file.loc[0,'w_l']\n",
    "        self.reward_file = reward_file\n",
    "        self.collision_file = collision_file\n",
    "        self.track_headings = self.calculate_track_headings(self.map_centers)\n",
    "        self.track_center_counter = self.map_centers.shape[0]  # Number of centers in the track\n",
    "\n",
    "        # Random Seed\n",
    "        self.random_seed = 42\n",
    "        np.random.seed(self.random_seed)\n",
    "\n",
    "        # Environment Observation Parameters\n",
    "        self.num_beams = 1080\n",
    "        self.n_features = 11\n",
    "        self.angle = 220\n",
    "\n",
    "        # LiDAR downsampling parameters\n",
    "        self.n_sectors = 40\n",
    "        self.normalized_lidar = np.zeros((1,self.n_sectors))\n",
    "\n",
    "        # Action Space Parameters\n",
    "        self.num_angles = self.n_sectors\n",
    "        \n",
    "\n",
    "        # State Space Parameters\n",
    "        self.num_states = 2 ** self.n_features\n",
    "\n",
    "        # Speed Parameters\n",
    "        self.min_speed = 0.8\n",
    "        self.max_speed = 1.8\n",
    "        self.num_speeds = 1\n",
    "\n",
    "        # Action Space\n",
    "        self.angles_deg = np.linspace(-self.angle // 2, self.angle // 2, self.num_angles)[::-1]\n",
    "        self.angles = np.radians(self.angles_deg)\n",
    "        self.fixed_speed = 1.0\n",
    "        # self.speeds = np.linspace(self.min_speed, self.max_speed, self.num_speeds)\n",
    "    \n",
    "        # State Space - Q-Table\n",
    "        if inference is not None:\n",
    "            self.weights = np.load(inference)\n",
    "            self.num_collisions = int(inference.split('_')[-1].split('.')[0])\n",
    "            print(f'Loaded Weights')\n",
    "        else:\n",
    "            self.weights = np.random.randn(self.num_states,self.num_angles)\n",
    "            self.num_collisions = 0\n",
    "        \n",
    "        self.max_weight = 5\n",
    "\n",
    "        # ELigibility Trace\n",
    "        self.ET_IS = np.zeros((self.num_states,self.num_angles))\n",
    "\n",
    "        # projection matrix\n",
    "        if self.n_features == 10:\n",
    "            zero_prob = 0.85\n",
    "            one_prob = 0.15\n",
    "        if self.n_features == 11:\n",
    "            zero_prob = 0.8\n",
    "            one_prob = 0.2\n",
    "\n",
    "        # zero_prob = 0.5\n",
    "        # one_prob = 0.5\n",
    "        \n",
    "        self.projection_matrix = self.get_projection_matrix(zero_prob=zero_prob,one_prob=one_prob)\n",
    "        # self.bias = np.linspace(-1,1,self.n_features).reshape(1,-1)\n",
    "        self.bias = np.zeros((1,self.n_features))\n",
    "\n",
    "        # binary powers\n",
    "        self.binary_powers = np.array([2 ** i for i in range(self.n_features)])\n",
    "\n",
    "        # Training Variables\n",
    "        self.curr_state = None\n",
    "        self.next_state = None\n",
    "        \n",
    "        self.action_threshold_decay = 0.9998\n",
    "        self.action_threshold = 0.1 * (self.action_threshold_decay ** self.num_collisions)\n",
    "\n",
    "        # Imported Classes\n",
    "        self.reward_class = Reward(min_speed=self.min_speed, max_speed=self.max_speed, num_speeds=self.num_speeds,map_centers=self.map_centers, track_width=self.track_width,logger = self.logger)\n",
    "        self.index_selector = IndexSelector(self.map_centers.shape[0])      \n",
    "\n",
    "        # SARSA Parameters\n",
    "        self.learning_rate = 1e-3\n",
    "        self.discount_factor = 0.95\n",
    "        self.decay_rate = 0.9\n",
    "\n",
    "        # Reward\n",
    "        self.reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.episodic_rewards = [0]\n",
    "\n",
    "        # Time\n",
    "        self.collision_times = [0]       \n",
    "\n",
    "\n",
    "    def calculate_track_headings(self,track_centers, window_size=5):\n",
    "        \"\"\"\n",
    "        Calculates orientations for track traversal.\n",
    "        \n",
    "        Args:\n",
    "            track_centers (np.ndarray): Shape (N, 2) array of track center points (x, y)\n",
    "            window_size (int): Number of points to consider for smoothing\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Shape (N,) array of orientation angles in radians\n",
    "        \"\"\"\n",
    "        num_points = track_centers.shape[0]\n",
    "        half_window = window_size // 2\n",
    "        \n",
    "        # Create indices for the future points (with wraparound)\n",
    "        future_indices = (np.arange(num_points) + half_window) % num_points\n",
    "        \n",
    "        # Get the future points\n",
    "        future_points = track_centers[future_indices]\n",
    "        \n",
    "        # Calculate direction vectors\n",
    "        direction_vectors = future_points - track_centers\n",
    "        \n",
    "        # Calculate angles using arctan2\n",
    "        orientations = np.arctan2(direction_vectors[:, 1], direction_vectors[:, 0])\n",
    "        \n",
    "        return orientations\n",
    "\n",
    "    def __update_map(self):\n",
    "        '''\n",
    "        Update the map of the environment to the next map in the list.\n",
    "        This function is called after fixed no. of collision with the environment, usually set to 2000 in the training loop.\n",
    "\n",
    "        '''\n",
    "        if self.env.renderer is not None:\n",
    "            self.env.renderer.close()\n",
    "        self.path_counter += 1\n",
    "        if self.path_counter == len(self.map_path):\n",
    "            self.path_counter = 0\n",
    "        self.env.map_name = self.map_path[self.path_counter]\n",
    "        self.env.update_map(f'{self.map_path[self.path_counter]}.yaml',self.map_ext)\n",
    "        F110Env.renderer = None\n",
    "        file = pd.read_csv(self.map_centers_file[self.path_counter])\n",
    "        file.columns = ['x', 'y', 'w_r', 'w_l']\n",
    "        file.index = file.index.astype(int)\n",
    "        self.map_centers = file.values[:, :2]\n",
    "        self.track_width = file.loc[0,'w_r'] + file.loc[0,'w_l']\n",
    "        self.track_headings = self.calculate_track_headings(self.map_centers)\n",
    "        self.track_center_counter = self.map_centers.shape[0]  # Reset the counter to the number of centers\n",
    "        print(f'Map updated to {self.track_name[self.path_counter]}')\n",
    "        self.logger.info('-------'*20)\n",
    "        self.logger.info(f'Initializing F1Tenth_navigation with map: {self.map_path[self.path_counter]}')\n",
    "        \n",
    "        \n",
    "    def render_callback(self, env_renderer):\n",
    "        e = env_renderer\n",
    "        x = e.cars[0].vertices[::2]\n",
    "        y = e.cars[0].vertices[1::2]\n",
    "        top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "        e.score_label.x = left\n",
    "        e.score_label.y = top - 700\n",
    "        e.left = left - 800\n",
    "        e.right = right + 800\n",
    "        e.top = top + 800\n",
    "        e.bottom = bottom - 800\n",
    "    \n",
    "    def calculate_track_headings(self,track_centers, window_size=5):\n",
    "        \"\"\"\n",
    "        Calculates orientations for track traversal.\n",
    "        \n",
    "        Args:\n",
    "            track_centers (np.ndarray): Shape (N, 2) array of track center points (x, y)\n",
    "            window_size (int): Number of points to consider for smoothing\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Shape (N,) array of orientation angles in radians\n",
    "        \"\"\"\n",
    "        num_points = track_centers.shape[0]\n",
    "        half_window = window_size // 2\n",
    "        \n",
    "        # Create indices for the future points (with wraparound)\n",
    "        future_indices = (np.arange(num_points) + half_window) % num_points\n",
    "        \n",
    "        # Get the future points\n",
    "        future_points = track_centers[future_indices]\n",
    "        \n",
    "        # Calculate direction vectors\n",
    "        direction_vectors = future_points - track_centers\n",
    "        \n",
    "        # Calculate angles using arctan2\n",
    "        orientations = np.arctan2(direction_vectors[:, 1], direction_vectors[:, 0])\n",
    "        \n",
    "        return orientations\n",
    "\n",
    "\n",
    "    def get_statistical_properties(self,lidar_input,n_sectors=None):\n",
    "        '''\n",
    "        Function that is used to downsample the Lidar input.\n",
    "        This function takes the Lidar input and divides it into n_sectors number of sectors.\n",
    "        It then calculates the median of each sector and returns the median values.\n",
    "        Args:\n",
    "            lidar_input (np.ndarray): Lidar input.\n",
    "            n_sectors (int): Number of sectors to downsample the Lidar input into.\n",
    "        '''\n",
    "        assert n_sectors is not None, \"Number of sectors must be provided\"\n",
    "        #  The [100 :-100] is for selecting only those rays corresponding to 220 fov.\n",
    "        sector_size = np.asarray(lidar_input[100:-100],dtype=np.float32).shape[0] // n_sectors\n",
    "        sectors = lidar_input[:sector_size * n_sectors].reshape(n_sectors, sector_size)\n",
    "        return np.median(sectors, axis=1).reshape(1,-1)\n",
    "    \n",
    "    def binarize_vector(self,vector):\n",
    "        '''\n",
    "        Function that is used to binarize the input.\n",
    "        This function takes the projected downsampled-Lidar and binarizes it based on the threshold.\n",
    "        Args:\n",
    "            vector (np.ndarray): Projected downsampled-Lidar input.\n",
    "        Returns:\n",
    "            np.ndarray: Binary represnetation of the Lidar data which is used as a state.\n",
    "        '''\n",
    "\n",
    "        threshold = (np.min(vector)+ np.max(vector))/2\n",
    "        return np.where(vector > threshold, 1, 0)\n",
    "        # return np.where(vector > 0, 1, 0)\n",
    "\n",
    "    def get_projection_matrix(self,zero_prob=0.5,one_prob=0.5):\n",
    "        '''\n",
    "        Function that is used to generate the projection matrix.\n",
    "        This function takes the number of features and the number of angles and generates a random projection matrix.\n",
    "        This function is called only once to generate the projection matrix and saves it to a file.\n",
    "        Args:\n",
    "            zero_prob (float): Probability of selecting 0.\n",
    "            one_prob (float): Probability of selecting 1.\n",
    "        Returns:\n",
    "            np.ndarray: Projection matrix.\n",
    "        '''\n",
    "        # Generate a random matrix with values 0 and 1 based on the given probabilities [prob_0,prob_1]\n",
    "        if not os.path.exists('Projection_matrices'):\n",
    "            os.mkdir('Projection_matrices')\n",
    "        if not os.path.exists(os.path.join('Projection_matrices', f'projection_{self.n_features}f_{self.num_angles}a_s{self.random_seed}.npy')):\n",
    "            # std = np.sqrt(1/self.n_features)\n",
    "            # matrix = np.random.choice([-1/std, 1/std], size=(self.n_sectors,self.n_features),p=[zero_prob, one_prob])\n",
    "            matrix = np.random.choice([0, 1], size=(self.n_sectors, self.n_features), p=[zero_prob,one_prob])\n",
    "            # matrix = np.random.normal(loc=0.0, scale=1/std, size=(self.n_sectors, self.n_features))\n",
    "            np.save(os.path.join('Projection_matrices', f'projection_{self.n_features}f_{self.num_angles}a_s{self.random_seed}.npy'), matrix)\n",
    "        else:\n",
    "            matrix = np.load(os.path.join('Projection_matrices', f'projection_{self.n_features}f_{self.num_angles}a_s{self.random_seed}.npy'))\n",
    "        return matrix\n",
    "\n",
    "    def get_binary_representation(self,lidar_input):\n",
    "        '''\n",
    "        Function that is used to get the binary representation of the Lidar input.\n",
    "        This function takes the Lidar input and projects it using the projection matrix.\n",
    "        It then binarizes the projected Lidar input and returns the binary representation.\n",
    "        the bias used is here is some types of non linear projections. It is set to zero here.\n",
    "        Args:\n",
    "            lidar_input (np.ndarray): Lidar input.\n",
    "        Returns:\n",
    "            np.ndarray: Binary representation of the Lidar input.\n",
    "        '''\n",
    "        self.normalized_lidar = normalize(lidar_input,axis=1)\n",
    "        # Do not normalize, just use the raw data\n",
    "        return self.binarize_vector(np.dot(lidar_input,self.projection_matrix) + self.bias)\n",
    "    \n",
    "\n",
    "    def get_state(self, binary):\n",
    "        return np.dot(binary[0], self.binary_powers)\n",
    "    \n",
    "\n",
    "    def select_action(self, state):\n",
    "        random_number = np.random.rand()\n",
    "        if random_number < self.action_threshold:\n",
    "            angle_index = np.random.randint(0, self.num_angles)\n",
    "        else:\n",
    "            max_value = np.max(self.weights[state])\n",
    "            max_indices = np.argwhere(self.weights[state] == max_value)\n",
    "            angle_index  = np.random.choice(max_indices.flatten())\n",
    "        return angle_index\n",
    "\n",
    "    def select_action_inference(self, state):\n",
    "        max_indices = np.argwhere(self.weights[state] == np.max(self.weights[state]))\n",
    "        angle_index  = np.random.choice(max_indices.flatten())\n",
    "        return angle_index \n",
    "\n",
    "    def sarsa_weight_update(self,angle_idx,speed_idx,reward):\n",
    "        next_angle_idx = self.select_action(self.next_state)\n",
    "        delta = reward + self.discount_factor * self.weights[self.next_state,next_angle_idx] - self.weights[self.curr_state,angle_idx]\n",
    "\n",
    "        self.weights += self.learning_rate * delta * self.ET_IS\n",
    "        return next_angle_idx\n",
    "\n",
    "    def set_eligibility_traces(self,angle_idx,speed_idx):\n",
    "        self.ET_IS [self.curr_state,angle_idx] = 1\n",
    "\n",
    "    def decay_eligibility_traces(self):\n",
    "        self.ET_IS *= self.discount_factor * self.decay_rate\n",
    "\n",
    "    def save_reward_time(self):\n",
    "        if not os.path.exists(os.path.join(self.save_path)):\n",
    "            os.mkdir(os.path.join(self.save_path))\n",
    "        \n",
    "        if self.reward_file is not None:\n",
    "            r = np.append(np.load(self.reward_file), self.episodic_rewards)\n",
    "            t = np.append(np.load(self.collision_file), self.collision_times)\n",
    "            np.save(os.path.join(self.save_path, f'rewards.npy'), np.array(r))\n",
    "            np.save(os.path.join(self.save_path, f'times.npy'), np.array(t))\n",
    "        else:\n",
    "            np.save(os.path.join(self.save_path, f'rewards.npy'), np.array(self.episodic_rewards))\n",
    "            np.save(os.path.join(self.save_path, f'times.npy'), np.array(self.collision_times))\n",
    "\n",
    "    def save_weights(self):\n",
    "        if not os.path.exists(os.path.join(self.save_path)):\n",
    "            os.mkdir(os.path.join(self.save_path))\n",
    "        np.save(os.path.join(self.save_path, f'{self.track_name[self.path_counter]}_{self.num_collisions + 1}.npy'), self.weights)\n",
    "        # print(f'File saved')\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.sx, self.sy, self.track_headings[0]]]))\n",
    "        lidar = obs['scans'][0]\n",
    "        lidar_down_sampled = self.get_statistical_properties(lidar,n_sectors=self.n_sectors)\n",
    "        self.curr_state = self.get_state(self.get_binary_representation(lidar_down_sampled))\n",
    "        self.reward_class.reset(np.array([[self.sx, self.sy]]))\n",
    "        angle_index = self.select_action(self.curr_state)\n",
    "        start_time = time.time()\n",
    "        while True:\n",
    "            # np.save(f'./LiDAR_scans/scan_{self.curr_state}.npy',lidar)\n",
    "            steering_angle = self.angles[angle_index]\n",
    "            curr_x = obs['poses_x'][0]\n",
    "            curr_y = obs['poses_y'][0]\n",
    "            obs, step_reward, done, info = self.env.step(np.array([[steering_angle, self.fixed_speed]]))\n",
    "            lidar = obs['scans'][0]\n",
    "            lidar_down_sampled = self.get_statistical_properties(lidar,n_sectors=self.n_sectors)\n",
    "            self.next_state = self.get_state(self.get_binary_representation(lidar_down_sampled))\n",
    "\n",
    "            if done:\n",
    "                self.reward =-1000\n",
    "            else:           \n",
    "                self.reward = self.reward_class.calculate_reward(np.array([curr_x, curr_y]), np.array([obs['poses_x'][0], obs['poses_y'][0]]),self.fixed_speed)\n",
    "            \n",
    "            self.episode_reward += self.reward\n",
    "            self.set_eligibility_traces(angle_index,0)\n",
    "\n",
    "\n",
    "            if done:\n",
    "                self.track_center_counter-=1\n",
    "                self.logger.info(f'Collision: {self.num_collisions+1}, State: {self.curr_state}, Action: {angle_index}, Reward: {self.episode_reward}')\n",
    "                self.action_threshold *= self.action_threshold_decay\n",
    "                self.episodic_rewards.append(self.episode_reward)\n",
    "                self.episode_reward = 0\n",
    "                end_time = time.time()\n",
    "                self.collision_times.append(end_time - start_time)\n",
    "                start_time = end_time\n",
    "\n",
    "                self.num_collisions += 1\n",
    "\n",
    "                self.ET_IS.fill(0)\n",
    "  \n",
    "\n",
    "                # Obtaining a new random position on the track\n",
    "                random_idx = self.index_selector.select_index()\n",
    "                n_x, n_y = self.map_centers[random_idx]\n",
    "                delta_x, delta_y = np.random.uniform(-0.75, 0.75), np.random.uniform(-0.3, 0.3)\n",
    "                delta_theta = np.random.uniform(-0.2, 0.2)\n",
    "                n_theta = self.track_headings[random_idx]\n",
    "\n",
    "                # Sensing the new state\n",
    "                obs, step_reward, done, info = self.env.reset(np.array([[n_x + delta_x, n_y + delta_y, n_theta+delta_theta]]))\n",
    "                lidar = obs['scans'][0]\n",
    "                lidar_down_sampled = self.get_statistical_properties(lidar,n_sectors=self.n_sectors)\n",
    "                self.curr_state = self.get_state(self.get_binary_representation(lidar_down_sampled))\n",
    "                angle_index = self.select_action(self.curr_state)\n",
    "\n",
    "                # Resetting \n",
    "                self.reward_class.reset(np.array([[n_x + delta_x, n_y + delta_y]]))\n",
    "                \n",
    "                # Checkpoint\n",
    "                if (self.num_collisions+1) % 500 == 0:\n",
    "                    print(f'Collision: {self.num_collisions+1}, Time: {sum(self.collision_times)}, Reward: {sum(self.episodic_rewards)/len(self.episodic_rewards)}')\n",
    "                    self.save_reward_time()\n",
    "                    self.save_weights()\n",
    "                    self.episodic_rewards.clear()\n",
    "                    self.collision_times.clear()\n",
    "                    self.reward_file = os.path.join(self.save_path, f'rewards.npy')\n",
    "                    self.collision_file = os.path.join(self.save_path, f'times.npy')\n",
    "                    \n",
    "                if  self.track_center_counter == 0:\n",
    "                    print(f'Training on {self.track_name[self.path_counter]} Completed')\n",
    "                    self.__update_map()\n",
    "                    self.reward_class.set_parameters(self.map_centers,self.track_width)\n",
    "                    self.reward_class.reset(np.array([[self.sx, self.sy]]))\n",
    "                    self.env.reset(np.array([[self.sx, self.sy, self.track_headings[0]]]))\n",
    "                    self.index_selector.set_parameters(self.map_centers.shape[0])\n",
    "            \n",
    "            angle_index = self.sarsa_weight_update(angle_index,0,self.reward)\n",
    "            self.decay_eligibility_traces()\n",
    "            self.curr_state = self.next_state\n",
    "            # self.env.render(mode='human')\n",
    "\n",
    "    def inference(self):\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.sx, self.sy, self.track_headings[0]]]))\n",
    "        lidar = obs['scans'][0]\n",
    "        lidar_down_sampled = self.get_statistical_properties(lidar,n_sectors=self.n_sectors)\n",
    "        self.curr_state = self.get_state(self.get_binary_representation(lidar_down_sampled))\n",
    "        self.reward_class.reset(np.array([[self.sx, self.sy]]))\n",
    "        angle_index = self.select_action_inference(self.curr_state)\n",
    "        while not done:\n",
    "            steering_angle = self.angles[angle_index]\n",
    "            obs, step_reward, done, info = self.env.step(np.array([[steering_angle, self.fixed_speed]]))\n",
    "            lidar = obs['scans'][0]\n",
    "            lidar_down_sampled = self.get_statistical_properties(lidar,n_sectors=self.n_sectors)\n",
    "            self.next_state = self.get_state(self.get_binary_representation(lidar_down_sampled))\n",
    "            angle_index = self.select_action_inference(self.next_state)\n",
    "            self.curr_state = self.next_state\n",
    "            \n",
    "            self.env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Austin', 1102),\n",
       " ('Silverstone', 1178),\n",
       " ('Sochi', 1169),\n",
       " ('Monza', 1159),\n",
       " ('Sepang', 1108),\n",
       " ('YasMarina', 1110),\n",
       " ('Melbourne', 1060),\n",
       " ('Catalunya', 931),\n",
       " ('Nuerburgring', 1029),\n",
       " ('BrandsHatch', 781),\n",
       " ('Montreal', 872),\n",
       " ('Spa', 1401),\n",
       " ('Shanghai', 1090),\n",
       " ('MoscowRaceway', 813),\n",
       " ('SaoPaulo', 862),\n",
       " ('Mexico City', 860),\n",
       " ('Oschersleben', 739),\n",
       " ('Hockenheim', 914),\n",
       " ('Budapest', 876),\n",
       " ('Sakhir', 1082),\n",
       " ('Zandvoort', 864),\n",
       " ('Spielberg', 864)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './f1tenth_racetracks'\n",
    "all_map_paths=[]\n",
    "map_centers = []\n",
    "map_names = []\n",
    "track_lengths=[]\n",
    "for folder in os.listdir(path):\n",
    "    if folder not in ['README.md','.gitignore','convert.py','LICENSE','rename.py','.git']:\n",
    "        folder_name=folder\n",
    "        file_name=folder_name.replace(' ','')+'_map'\n",
    "        map_center = folder_name.replace(' ','')+'_centerline.csv'\n",
    "        track_lengths.append(len(pd.read_csv(f'{path}/{folder_name}/{map_center}')))\n",
    "        map_names.append(folder_name)\n",
    "        all_map_paths.append(f'{path}/{folder_name}/{file_name}')\n",
    "        map_centers.append(f'{path}/{folder_name}/{map_center}')\n",
    "\n",
    "track_length_list = list(zip(map_names,track_lengths))\n",
    "track_length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Maps: ['Spa', 'Hockenheim', 'Shanghai', 'Nuerburgring', 'Montreal', 'Austin', 'Mexico City']\n",
      "Test Maps: ['Silverstone', 'Sochi', 'Monza', 'Sepang', 'YasMarina', 'Melbourne', 'Catalunya', 'BrandsHatch', 'MoscowRaceway', 'SaoPaulo', 'Oschersleben', 'Budapest', 'Sakhir', 'Zandvoort', 'Spielberg']\n"
     ]
    }
   ],
   "source": [
    "train_maps = ['Spa','Hockenheim','Shanghai','Nuerburgring','Montreal', 'Austin','Mexico City']\n",
    "test_maps = [i[0] for i in track_length_list if i[0] not in train_maps]\n",
    "print(f'Train Maps: {train_maps}')\n",
    "print(f'Test Maps: {test_maps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/f1tenth_gym/gym/f110_gym/envs/base_classes.py:93: UserWarning: Chosen integrator is RK4. This is different from previous versions of the gym.\n",
      "  warnings.warn(f\"Chosen integrator is RK4. This is different from previous versions of the gym.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collision: 500, Time: 339.9062807559967, Reward: -4382.663340425388\n",
      "Collision: 1000, Time: 779.205436706543, Reward: -3034.092729807569\n",
      "Training on Austin Completed\n",
      "Map updated to Nuerburgring\n",
      "Collision: 1500, Time: 391.43050384521484, Reward: -5596.767684215462\n",
      "Collision: 2000, Time: 328.1725687980652, Reward: -6180.579744227346\n",
      "Training on Nuerburgring Completed\n",
      "Map updated to Montreal\n",
      "Collision: 2500, Time: 248.74094033241272, Reward: -2168.179147028007\n",
      "Collision: 3000, Time: 255.78688430786133, Reward: -896.9360357921375\n",
      "Training on Montreal Completed\n",
      "Map updated to Spa\n",
      "Collision: 3500, Time: 1112.0771877765656, Reward: -483.7969468465188\n",
      "Collision: 4000, Time: 1147.7087588310242, Reward: -218.15287862397452\n",
      "Training on Spa Completed\n",
      "Map updated to Shanghai\n",
      "Collision: 4500, Time: 1401.9236981868744, Reward: 401.9734367304405\n",
      "Collision: 5000, Time: 1171.51864361763, Reward: -1867.4721745041745\n",
      "Training on Shanghai Completed\n",
      "Map updated to Mexico City\n",
      "Collision: 5500, Time: 1131.128722190857, Reward: -5632.27304567145\n",
      "Collision: 6000, Time: 1320.29061961174, Reward: -364.42249104718167\n",
      "Training on Mexico City Completed\n",
      "Map updated to Hockenheim\n",
      "Collision: 6500, Time: 1128.493616104126, Reward: -1585.84116256241\n",
      "Collision: 7000, Time: 613.2667992115021, Reward: -3264.263786064382\n",
      "Training on Hockenheim Completed\n",
      "Map updated to Austin\n",
      "Collision: 7500, Time: 534.5706913471222, Reward: -2472.176235492608\n",
      "Collision: 8000, Time: 593.952080488205, Reward: -910.2186500185778\n",
      "Training on Austin Completed\n",
      "Map updated to Nuerburgring\n",
      "Collision: 8500, Time: 509.5038115978241, Reward: -3343.0872842418266\n",
      "Collision: 9000, Time: 867.0709137916565, Reward: -4905.088615881315\n",
      "Training on Nuerburgring Completed\n",
      "Map updated to Montreal\n",
      "Collision: 9500, Time: 1069.593008518219, Reward: -4267.059132789526\n",
      "Collision: 10000, Time: 861.1686823368073, Reward: 758.3815216282705\n",
      "Training on Montreal Completed\n",
      "Map updated to Spa\n",
      "Collision: 10500, Time: 1481.6941561698914, Reward: -377.8336350190164\n",
      "Collision: 11000, Time: 1229.3473732471466, Reward: -1199.0628505455888\n",
      "Collision: 11500, Time: 963.3059411048889, Reward: -502.95757347089915\n",
      "Training on Spa Completed\n",
      "Map updated to Shanghai\n",
      "Collision: 12000, Time: 707.8754260540009, Reward: -6610.4677057045\n",
      "Collision: 12500, Time: 509.4646050930023, Reward: -8749.678578609184\n",
      "Training on Shanghai Completed\n",
      "Map updated to Mexico City\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m map_names_subset \u001b[38;5;241m=\u001b[39m [map_names[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[1;32m     45\u001b[0m simulator \u001b[38;5;241m=\u001b[39m F1Tenth_navigation(gym_env_code\u001b[38;5;241m=\u001b[39mgym_env_code, num_agents\u001b[38;5;241m=\u001b[39mnum_agents, map_path\u001b[38;5;241m=\u001b[39mmap_path_subset, map_ext\u001b[38;5;241m=\u001b[39mmap_ext, sx\u001b[38;5;241m=\u001b[39msx, sy\u001b[38;5;241m=\u001b[39msy, stheta\u001b[38;5;241m=\u001b[39mstheta, map_centers_file\u001b[38;5;241m=\u001b[39mmap_centers_subset, save_path\u001b[38;5;241m=\u001b[39msave_path, track_name\u001b[38;5;241m=\u001b[39mmap_names_subset, inference\u001b[38;5;241m=\u001b[39minference_file,reward_file\u001b[38;5;241m=\u001b[39mreward_file,collision_file\u001b[38;5;241m=\u001b[39mcollision_file)\n\u001b[0;32m---> 46\u001b[0m \u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 351\u001b[0m, in \u001b[0;36mF1Tenth_navigation.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m curr_x \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposes_x\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    350\u001b[0m curr_y \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposes_y\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 351\u001b[0m obs, step_reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43msteering_angle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_speed\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m lidar \u001b[38;5;241m=\u001b[39m obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscans\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m lidar_down_sampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_statistical_properties(lidar,n_sectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_sectors)\n",
      "File \u001b[0;32m/f1tenth_gym/gym/f110_gym/envs/f110_env.py:270\u001b[0m, in \u001b[0;36mF110Env.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03mStep function for the gym env\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    info (dict): auxillary information dictionary\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# call simulation step\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlap_times\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlap_times\n\u001b[1;32m    272\u001b[0m obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlap_counts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlap_counts\n",
      "File \u001b[0;32m/f1tenth_gym/gym/f110_gym/envs/base_classes.py:563\u001b[0m, in \u001b[0;36mSimulator.step\u001b[0;34m(self, control_inputs)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# looping over agents\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents):\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;66;03m# update each agent's pose\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m     current_scan \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m     agent_scans\u001b[38;5;241m.\u001b[39mappend(current_scan)\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;66;03m# update sim's information of agent poses\u001b[39;00m\n",
      "File \u001b[0;32m/f1tenth_gym/gym/f110_gym/envs/base_classes.py:405\u001b[0m, in \u001b[0;36mRaceCar.update_pose\u001b[0;34m(self, raw_steer, vel)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# update scan\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m current_scan \u001b[38;5;241m=\u001b[39m \u001b[43mRaceCar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_simulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_rng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m current_scan\n",
      "File \u001b[0;32m/f1tenth_gym/gym/f110_gym/envs/laser_models.py:448\u001b[0m, in \u001b[0;36mScanSimulator2D.scan\u001b[0;34m(self, pose, rng, std_dev)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_height \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMap is not set for scan simulator.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m scan \u001b[38;5;241m=\u001b[39m \u001b[43mget_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta_dis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta_index_increment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rng \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     noise \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0.\u001b[39m, std_dev, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beams)\n",
      "File \u001b[0;32m/opt/my_env/lib/python3.10/site-packages/numba/core/serialize.py:30\u001b[0m, in \u001b[0;36m_numba_unpickle\u001b[0;34m(address, bytedata, hashed)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Keep unpickled object via `numba_unpickle` alive.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m _unpickled_memo \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_numba_unpickle\u001b[39m(address, bytedata, hashed):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Used by `numba_unpickle` from _helperlib.c\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        unpickled object\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     key \u001b[38;5;241m=\u001b[39m (address, hashed)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global num_agents,map_path,map_ext,sx,sy,stheta,num_maps_to_train,gym_env_code,inference_file,reward_file,collision_file,counter,indices,save_path\n",
    "gym_env_code='f110_gym:f110-v0'\n",
    "num_agents = 1\n",
    "map_ext = '.png'\n",
    "sx = 0.\n",
    "sy = 0.\n",
    "stheta = {'Hockenheim': 2.02,\n",
    " 'Mexico City': -0.15,\n",
    " 'Oschersleben': 2.86,\n",
    " 'Shanghai': -2.93,\n",
    " 'BrandsHatch': 0.42,\n",
    " 'Monza': 1.47,\n",
    " 'Catalunya': -2.14,\n",
    " 'SaoPaulo': -1.31,\n",
    " 'Sepang': -3.06,\n",
    " 'Silverstone': 0.94,\n",
    " 'Nuerburgring': -2.38,\n",
    " 'YasMarina': 0.13,\n",
    " 'Spa': 2.13,\n",
    " 'Sochi': -2.14,\n",
    " 'Montreal': -1.35,\n",
    " 'Austin': -0.65,\n",
    " 'Melbourne': 2.37,\n",
    " 'Budapest': 2.45,\n",
    " 'Spielberg': -2.88,\n",
    " 'Zandvoort': 1.2,\n",
    " 'Sakhir': 1.53,\n",
    " 'MoscowRaceway': 1.46}\n",
    "\n",
    "indices = [idx for idx,i in enumerate(map_names) if i in train_maps]\n",
    "save_path = 'SARSA_no_speed_binary_training/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "counter = 0\n",
    "# inference_file = 'SARSA_Fewer_Multiple_training/Mexico City_34500.npy'\n",
    "# reward_file='SARSA_Fewer_Multiple_training/rewards.npy'\n",
    "# collision_file='SARSA_Fewer_Multiple_training/times.npy'\n",
    "inference_file = None\n",
    "reward_file = None\n",
    "collision_file = None\n",
    "map_path_subset = [all_map_paths[i] for i in indices]\n",
    "map_centers_subset = [map_centers[i] for i in indices]\n",
    "map_names_subset = [map_names[i] for i in indices]\n",
    "\n",
    "simulator = F1Tenth_navigation(gym_env_code=gym_env_code, num_agents=num_agents, map_path=map_path_subset, map_ext=map_ext, sx=sx, sy=sy, stheta=stheta, map_centers_file=map_centers_subset, save_path=save_path, track_name=map_names_subset, inference=inference_file,reward_file=reward_file,collision_file=collision_file)\n",
    "simulator.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - Binary - Centerline + Speed\n",
    "- 360 min - 12.5k collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/workspaces/f1_tenth/'\n",
    "reward_file_BTSP = f'{path}BTSP_Multiple_training/rewards.npy'\n",
    "time_file_BTSP = f'{path}BTSP_Multiple_training/times.npy'\n",
    "path_BTSP = f'{path}BTSP_Multiple_training/'\n",
    "\n",
    "reward_file_q = f'{path}Q_Multiple_training/rewards.npy'\n",
    "time_file_q = f'{path}Q_Multiple_training/times.npy'\n",
    "path_q = f'{path}Q_Multiple_training/'\n",
    "\n",
    "\n",
    "reward_file_sarsa = f'{path}SARSA_Multiple_training/rewards.npy'\n",
    "time_file_sarsa = f'{path}SARSA_Multiple_training/times.npy'\n",
    "path_sarsa = f'{path}SARSA_Multiple_training/'\n",
    "\n",
    "episode_num_BTSP=[0]\n",
    "for file in os.listdir(path_BTSP):\n",
    "    if (not file.startswith('reward')) and (not file.startswith('times')) :\n",
    "        episode_num_BTSP.append(int(int(file.split('_')[1].split('.')[0])))\n",
    "\n",
    "episode_num_Q=[0]\n",
    "for file in os.listdir(path_q):\n",
    "    if (not file.startswith('reward')) and (not file.startswith('times')) :\n",
    "        episode_num_Q.append(int(int(file.split('_')[1].split('.')[0])))\n",
    "\n",
    "episode_num_sarsa=[0]\n",
    "for file in os.listdir(path_sarsa):\n",
    "    if (not file.startswith('reward')) and (not file.startswith('times')) :\n",
    "        episode_num_sarsa.append(int(int(file.split('_')[1].split('.')[0])))\n",
    "\n",
    "\n",
    "episode_num_Q = sorted(episode_num_Q) \n",
    "episode_num_sarsa = sorted(episode_num_sarsa) \n",
    "episode_num_BTSP = sorted(episode_num_BTSP) \n",
    "\n",
    "reward = np.load(reward_file_BTSP)\n",
    "times= np.load(time_file_BTSP)\n",
    "\n",
    "reward_q = np.load(reward_file_q)\n",
    "times_q= np.load(time_file_q)\n",
    "\n",
    "reward_sarsa = np.load(reward_file_sarsa)\n",
    "times_sarsa = np.load(time_file_sarsa)\n",
    "\n",
    "min_episode_num = min(episode_num_BTSP[-1],episode_num_Q[-1],episode_num_sarsa[-1])\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "ax[0].plot(list(range(min_episode_num)),reward[:min_episode_num],label='HCL')\n",
    "ax[0].plot(list(range(min_episode_num)),reward_q[:min_episode_num],label='Q($\\lambda$)')\n",
    "ax[0].plot(list(range(min_episode_num)),reward_sarsa[:min_episode_num],label='SARSA($\\lambda$)')\n",
    "\n",
    "ax[0].set_title('Reward Vs Trials')\n",
    "ax[0].set_xlabel('Number of Trials')\n",
    "ax[0].set_ylabel('Reward') \n",
    "\n",
    "ax[1].plot(list(range(min_episode_num)),times[:min_episode_num],label='HCL')\n",
    "ax[1].plot(list(range(min_episode_num)),times_q[:min_episode_num],label='Q($\\lambda$)')\n",
    "ax[1].plot(list(range(min_episode_num)),times_sarsa[:min_episode_num],label='SARSA($\\lambda$)')\n",
    "\n",
    "ax[1].set_title('Time Vs Trials')\n",
    "ax[1].set_xlabel('Number of Trials')\n",
    "ax[1].set_ylabel('Time to Collisions s')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "# fig.suptitle('BTSP')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/workspaces/f1_tenth/'\n",
    "path = '/home/praneeth/shared_f1_tenth /'\n",
    "reward_file_BTSP = f'{path}BTSP_220_Multiple_training/rewards.npy'\n",
    "time_file_BTSP = f'{path}BTSP_220_Multiple_training/times.npy'\n",
    "path_BTSP = f'{path}BTSP_220_Multiple_training/'\n",
    "\n",
    "# reward_file_q = f'{path}Q_Multiple_training/rewards.npy'\n",
    "# time_file_q = f'{path}Q_Multiple_training/times.npy'\n",
    "# path_q = f'{path}Q_Multiple_training/'\n",
    "\n",
    "\n",
    "reward_file_sarsa = f'{path}SARSA_220_Multiple_training/rewards.npy'\n",
    "time_file_sarsa = f'{path}SARSA_220_Multiple_training/times.npy'\n",
    "path_sarsa = f'{path}SARSA_220_Multiple_training/'\n",
    "\n",
    "episode_num_BTSP=[0]\n",
    "for file in os.listdir(path_BTSP):\n",
    "    if (not file.startswith('reward')) and (not file.startswith('times')) :\n",
    "        episode_num_BTSP.append(int(int(file.split('_')[1].split('.')[0])))\n",
    "\n",
    "# episode_num_Q=[0]\n",
    "# for file in os.listdir(path_q):\n",
    "#     if (not file.startswith('reward')) and (not file.startswith('times')) :\n",
    "#         episode_num_Q.append(int(int(file.split('_')[1].split('.')[0])))\n",
    "\n",
    "episode_num_sarsa=[0]\n",
    "for file in os.listdir(path_sarsa):\n",
    "    if (not file.startswith('reward')) and (not file.startswith('times')) :\n",
    "        episode_num_sarsa.append(int(int(file.split('_')[1].split('.')[0])))\n",
    "\n",
    "\n",
    "# episode_num_Q = sorted(episode_num_Q) \n",
    "episode_num_sarsa = sorted(episode_num_sarsa) \n",
    "episode_num_BTSP = sorted(episode_num_BTSP) \n",
    "\n",
    "reward = np.load(reward_file_BTSP)\n",
    "times= np.load(time_file_BTSP)\n",
    "\n",
    "# reward_q = np.load(reward_file_q)\n",
    "# times_q= np.load(time_file_q)\n",
    "\n",
    "reward_sarsa = np.load(reward_file_sarsa)\n",
    "times_sarsa = np.load(time_file_sarsa)\n",
    "\n",
    "# min_episode_num = min(episode_num_BTSP[-1],episode_num_Q[-1],episode_num_sarsa[-1])\n",
    "min_episode_num = min(episode_num_BTSP[-1],episode_num_sarsa[-1])\n",
    "\n",
    "global aggregration_size \n",
    "aggregration_size = 1000\n",
    "\n",
    "def aggregate_and_average(data, aggregation_size=aggregration_size):\n",
    "    \"\"\"Aggregates data by summing every 'aggregation_size' values and then averaging.\"\"\"\n",
    "    aggregated_data = []\n",
    "    for i in range(0, len(data), aggregation_size):\n",
    "        chunk = data[i:i + aggregation_size]\n",
    "        if len(chunk) > 0:\n",
    "            aggregated_data.append(np.sum(chunk)) \n",
    "    return aggregated_data\n",
    "\n",
    "aggregated_reward_BTSP = aggregate_and_average(reward[:min_episode_num])\n",
    "aggregated_times_BTSP = aggregate_and_average(times[:min_episode_num])\n",
    "# aggregated_reward_q = aggregate_and_average(reward_q[:min_episode_num])\n",
    "# aggregated_times_q = aggregate_and_average(times_q[:min_episode_num])\n",
    "aggregated_reward_sarsa = aggregate_and_average(reward_sarsa[:min_episode_num])\n",
    "aggregated_times_sarsa = aggregate_and_average(times_sarsa[:min_episode_num])\n",
    "\n",
    "aggregated_episode_num_BTSP = list(range(0, min_episode_num, aggregration_size))[:len(aggregated_reward_BTSP)]\n",
    "# aggregated_episode_num_Q = list(range(0, min_episode_num, aggregration_size))[:len(aggregated_reward_q)]\n",
    "aggregated_episode_num_sarsa = list(range(0, min_episode_num, aggregration_size))[:len(aggregated_reward_sarsa)]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].plot(aggregated_episode_num_BTSP, aggregated_reward_BTSP, label='HCL')\n",
    "# ax[0].plot(aggregated_episode_num_Q, aggregated_reward_q, label='Q($\\lambda$)')\n",
    "ax[0].plot(aggregated_episode_num_sarsa, aggregated_reward_sarsa, label='SARSA($\\lambda$)')\n",
    "ax[0].set_title('Reward Vs Trials (Aggregated)')\n",
    "ax[0].set_xlabel(f'Number of Trials (Groups of {aggregration_size})')\n",
    "ax[0].set_ylabel('Average Reward')\n",
    "\n",
    "ax[1].plot(aggregated_episode_num_BTSP, aggregated_times_BTSP, label='HCL')\n",
    "# ax[1].plot(aggregated_episode_num_Q, aggregated_times_q, label='Q($\\lambda$)')\n",
    "ax[1].plot(aggregated_episode_num_sarsa, aggregated_times_sarsa, label='SARSA($\\lambda$)')\n",
    "\n",
    "ax[1].set_title('Time Vs Trials (Aggregated)')\n",
    "ax[1].set_xlabel(f'Number of Trials (Groups of {aggregration_size})')\n",
    "ax[1].set_ylabel('Time to Collisions(s)')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/praneeth/shared_f1_tenth /'\n",
    "reward_file_normal = f'{path}Weights/{folder_name}/rewards.npy'\n",
    "time_file_normal = f'{path}Weights/{folder_name}/times.npy'\n",
    "path_normal = f'{path}Weights/{folder_name}'\n",
    "episode_num=[0]\n",
    "for file in os.listdir(path):\n",
    "    if file.startswith('weights'):\n",
    "        episode_num.append(int(int(file.split('_')[1].split('.')[0])/1000))\n",
    "\n",
    "episode_num = sorted(episode_num) \n",
    "\n",
    "reward = np.load(reward_file_normal)\n",
    "times= np.load(time_file_normal)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "ax[0].plot(list(range(episode_num[-1]+1)[:21]),reward[:21]/10000)\n",
    "ax[0].set_title('Reward Vs Collisions')\n",
    "ax[0].set_xlabel('Number of collisions ($10^3$)')\n",
    "ax[0].set_ylabel('Reward ($10^4$)') \n",
    "ax[1].plot(list(range(episode_num[-1]+1)[:21]),times[:21]/1000)\n",
    "ax[1].set_title('Time Vs Collisions')\n",
    "ax[1].set_xlabel('Number of collisions ($10^3$)')\n",
    "ax[1].set_ylabel('Time to collision ($10^3$) s')\n",
    "fig.suptitle('SARSA($\\lambda$)')\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([aggregated_times_BTSP,aggregated_times_sarsa,aggregated_times_q], labels=['HCL', 'SARSA', 'Q-learning'])\n",
    "plt.title('Time to collision (Training)')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = plt.boxplot([aggregated_times_BTSP, aggregated_times_sarsa, aggregated_times_q],labels=['HCL', 'SARSA($\\lambda$)', 'Q-learning'])\n",
    "\n",
    "# Extract the coordinates of the upper whiskers\n",
    "# whiskers = boxplot['whiskers']\n",
    "# # Upper whiskers are at indices 1, 3, 5 (every second whisker is an upper whisker)\n",
    "# upper_whisker_xs = [1, 2, 3]  # x-coordinates of the boxes\n",
    "# upper_whisker_ys = [whiskers[i].get_ydata()[1] for i in range(1, 6, 2)]  # y values of upper whiskers\n",
    "\n",
    "# # Connect the upper whiskers with a line\n",
    "# plt.plot(upper_whisker_xs, upper_whisker_ys, 'r--', linewidth=1.5)\n",
    "\n",
    "# Customize the plot if needed\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.title('Time to $N$ collisions (Training)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aggregated_episode_num_BTSP, aggregated_times_BTSP, label='HCL')\n",
    "plt.plot(aggregated_episode_num_Q, aggregated_times_q, label='Q-learning')\n",
    "plt.plot(aggregated_episode_num_sarsa, aggregated_times_sarsa, label='SARSA($\\lambda$)')\n",
    "plt.title('Avg. time to collision progression (Training)')\n",
    "plt.xlabel('Number of Collisions / Trials')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
