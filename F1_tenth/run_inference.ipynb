{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from f110_gym.envs.base_classes import Integrator\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "import sys\n",
    "from f110_gym.envs.f110_env import F110Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fewer actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Tenth_navigation:\n",
    "\n",
    "    def __init__(self,gym_env_code='f110_gym:f110-v0', num_agents=1, map_path=['./f1tenth_racetracks/Austin/Austin_map'], map_ext='.png', sx=0., sy=0., stheta=None, map_centers_file=None, save_path=None, track_name=None, inference=None,reward_file=None,collision_file=None):\n",
    "\n",
    "        # Environment setup\n",
    "        self.path_counter = 0\n",
    "        self.sx, self.sy, self.stheta = sx, sy, stheta\n",
    "        self.save_path = save_path\n",
    "        self.track_name = track_name\n",
    "        self.num_agents = num_agents\n",
    "        self.map_path = map_path\n",
    "        self.map_ext = map_ext\n",
    "        self.map_centers_file = map_centers_file\n",
    "\n",
    "    \n",
    "        self.env = gym.make(gym_env_code, map=self.map_path[self.path_counter], map_ext=self.map_ext, num_agents=self.num_agents, timestep=0.01, integrator=Integrator.RK4)\n",
    "        self.env.add_render_callback(self.render_callback)\n",
    "       \n",
    "        file = pd.read_csv(self.map_centers_file[self.path_counter])\n",
    "        file.columns = ['x', 'y', 'w_r', 'w_l']\n",
    "        file.index = file.index.astype(int)\n",
    "        self.map_centers = file.values[:, :2]\n",
    "        self.track_width = file.loc[0,'w_r'] + file.loc[0,'w_l']\n",
    "        self.reward_file = reward_file\n",
    "        self.track_headings = self.calculate_track_headings(self.map_centers)\n",
    "        self.collision_file = collision_file\n",
    "\n",
    "        # Random Seed\n",
    "        self.random_seed = 42\n",
    "        np.random.seed(self.random_seed)\n",
    "\n",
    "        # Environment Observation Parameters\n",
    "        self.num_beams = 1080\n",
    "        self.n_features = 11\n",
    "        self.angle = 220\n",
    "\n",
    "        # LiDAR downsampling parameters\n",
    "        self.n_sectors = 22\n",
    "        self.normalized_lidar = np.zeros((1,self.n_sectors))\n",
    "\n",
    "        # Action Space Parameters\n",
    "        self.num_angles = self.n_sectors\n",
    "        self.num_speeds = 5\n",
    "\n",
    "\n",
    "        # State Space Parameters\n",
    "        self.num_states = 2 ** self.n_features\n",
    "\n",
    "        # Speed Parameters\n",
    "        self.min_speed = 0.8\n",
    "        self.max_speed = 1.8\n",
    "\n",
    "        # Action Space\n",
    "        self.angles_deg = np.linspace(-self.angle // 2, self.angle // 2, self.num_angles)[::-1]\n",
    "        self.angles = np.radians(self.angles_deg)\n",
    "        self.speeds = np.linspace(self.min_speed, self.max_speed, self.num_speeds)\n",
    "    \n",
    "        # State Space - Q-Table\n",
    "        if inference is not None:\n",
    "            self.weights = np.load(inference)\n",
    "            self.num_collisions = int(inference.split('_')[-1].split('.')[0])\n",
    "            print(f'Loaded Weights')\n",
    "        else:\n",
    "            self.weights = np.zeros((self.num_states,self.num_angles,self.num_speeds))\n",
    "            self.num_collisions = 0\n",
    "        \n",
    "        self.max_weight = 5\n",
    "\n",
    "        # ELigibility Trace\n",
    "        self.ET = np.zeros((1,self.num_states))\n",
    "        self.IS = np.zeros((self.num_angles,self.num_speeds))\n",
    "\n",
    "        # projection matrix\n",
    "        if self.n_features == 10:\n",
    "            zero_prob = 0.85\n",
    "            one_prob = 0.15\n",
    "        if self.n_features == 11:\n",
    "            zero_prob = 0.8\n",
    "            one_prob = 0.2\n",
    "        self.projection_matrix = self.get_projection_matrix(zero_prob=zero_prob,one_prob=one_prob)\n",
    "        # self.bias = np.linspace(-0.5,0.5,self.n_features).reshape(1,-1)\n",
    "        self.bias = np.zeros((1,self.n_features))\n",
    "\n",
    "        # binary powers\n",
    "        self.binary_powers = np.array([2 ** i for i in range(self.n_features)])\n",
    "\n",
    "        # Training Variables\n",
    "        self.curr_state = None\n",
    "        self.next_state = None\n",
    "        \n",
    "        self.action_threshold_decay = 0.9998\n",
    "        self.action_threshold = 0.1 * (self.action_threshold_decay ** self.num_collisions)\n",
    "\n",
    "        # Imported Classes\n",
    "        # self.reward_class = Reward(min_speed=self.min_speed, max_speed=self.max_speed, map_centers=self.map_centers, track_width=self.track_width)\n",
    "        # self.index_selector = IndexSelector(self.map_centers.shape[0])      \n",
    "\n",
    "        # BTSP Parameters\n",
    "        self.learning_rate = 1e-3\n",
    "        self.ET_decay_rate = 0.9\n",
    "        self.IS_decay_rate = 0.7\n",
    "\n",
    "        # Reward\n",
    "        self.reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.episodic_rewards = [0]\n",
    "\n",
    "        # Time\n",
    "        self.collision_times = [0]       \n",
    "\n",
    "\n",
    "    def calculate_track_headings(self,track_centers, window_size=5):\n",
    "        \"\"\"\n",
    "        Calculates orientations for track traversal.\n",
    "        \n",
    "        Args:\n",
    "            track_centers (np.ndarray): Shape (N, 2) array of track center points (x, y)\n",
    "            window_size (int): Number of points to consider for smoothing\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Shape (N,) array of orientation angles in radians\n",
    "        \"\"\"\n",
    "        num_points = track_centers.shape[0]\n",
    "        half_window = window_size // 2\n",
    "        \n",
    "        # Create indices for the future points (with wraparound)\n",
    "        future_indices = (np.arange(num_points) + half_window) % num_points\n",
    "        \n",
    "        # Get the future points\n",
    "        future_points = track_centers[future_indices]\n",
    "        \n",
    "        # Calculate direction vectors\n",
    "        direction_vectors = future_points - track_centers\n",
    "        \n",
    "        # Calculate angles using arctan2\n",
    "        orientations = np.arctan2(direction_vectors[:, 1], direction_vectors[:, 0])\n",
    "        \n",
    "        return orientations\n",
    "    \n",
    "    def __update_map(self):\n",
    "        if self.env.renderer is not None:\n",
    "            self.env.renderer.close()\n",
    "        self.path_counter += 1\n",
    "        if self.path_counter == len(self.map_path):\n",
    "            self.path_counter = 0\n",
    "        self.env.map_name = self.map_path[self.path_counter]\n",
    "        self.env.update_map(f'{self.map_path[self.path_counter]}.yaml',self.map_ext)\n",
    "        F110Env.renderer = None\n",
    "        file = pd.read_csv(self.map_centers_file[self.path_counter])\n",
    "        file.columns = ['x', 'y', 'w_r', 'w_l']\n",
    "        file.index = file.index.astype(int)\n",
    "        self.map_centers = file.values[:, :2]\n",
    "        self.track_width = file.loc[0,'w_r'] + file.loc[0,'w_l']\n",
    "        self.track_headings = self.calculate_track_headings(self.map_centers)\n",
    "        print(f'Map updated to {self.track_name[self.path_counter]}')\n",
    "        \n",
    "        \n",
    "    def render_callback(self, env_renderer):\n",
    "        e = env_renderer\n",
    "        x = e.cars[0].vertices[::2]\n",
    "        y = e.cars[0].vertices[1::2]\n",
    "        top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "        e.score_label.x = left\n",
    "        e.score_label.y = top - 700\n",
    "        e.left = left - 800\n",
    "        e.right = right + 800\n",
    "        e.top = top + 800\n",
    "        e.bottom = bottom - 800\n",
    "\n",
    "\n",
    "    def get_statistical_properties(self,lidar_input,n_sectors=None):\n",
    "        assert n_sectors is not None, \"Number of sectors must be provided\"\n",
    "        #  The [100 :-100] is for selecting only those rays corresponding to 220 fov.\n",
    "        sector_size = np.asarray(lidar_input[100:-100],dtype=np.float32).shape[0] // n_sectors\n",
    "        sectors = lidar_input[:sector_size * n_sectors].reshape(n_sectors, sector_size)\n",
    "        return np.median(sectors, axis=1).reshape(1,-1)\n",
    "    \n",
    "    def binarize_vector(self,vector):\n",
    "        '''\n",
    "        Function that is used to binarize the input.\n",
    "        This function takes the projected downsampled-Lidar and binarizes it based on the threshold.\n",
    "        Args:\n",
    "            vector (np.ndarray): Projected downsampled-Lidar input.\n",
    "        Returns:\n",
    "            np.ndarray: Binary represnetation of the Lidar data which is used as a state.\n",
    "        '''\n",
    "\n",
    "        # threshold = (np.min(vector)+ np.max(vector))/2\n",
    "        # return np.where(vector > threshold, 1, 0)\n",
    "        return np.where(vector > 0, 1, 0)\n",
    "\n",
    "    def get_projection_matrix(self,zero_prob=0.5,one_prob=0.5):\n",
    "        '''\n",
    "        Function that is used to generate the projection matrix.\n",
    "        This function takes the number of features and the number of angles and generates a random projection matrix.\n",
    "        This function is called only once to generate the projection matrix and saves it to a file.\n",
    "        Args:\n",
    "            zero_prob (float): Probability of selecting 0.\n",
    "            one_prob (float): Probability of selecting 1.\n",
    "        Returns:\n",
    "            np.ndarray: Projection matrix.\n",
    "        '''\n",
    "        # Generate a random matrix with values 0 and 1 based on the given probabilities [prob_0,prob_1]\n",
    "        if not os.path.exists('Projection_matrices'):\n",
    "            os.mkdir('Projection_matrices')\n",
    "        if not os.path.exists(os.path.join('Projection_matrices', f'projection_{self.n_features}f_{self.num_angles}a_s{self.random_seed}.npy')):\n",
    "            std = np.sqrt(1/self.n_features)\n",
    "            # matrix = np.random.choice([-1/std, 1/std], size=(self.n_sectors,self.n_features),p=[zero_prob, one_prob])\n",
    "            # matrix = np.random.choice([0, 1], size=(self.n_sectors, self.n_features), p=[zero_prob,one_prob])\n",
    "            matrix = np.random.normal(loc=0.0, scale=1/std, size=(self.n_sectors, self.n_features))\n",
    "            np.save(os.path.join('Projection_matrices', f'projection_{self.n_features}f_{self.num_angles}a_s{self.random_seed}.npy'), matrix)\n",
    "        else:\n",
    "            matrix = np.load(os.path.join('Projection_matrices', f'projection_{self.n_features}f_{self.num_angles}a_s{self.random_seed}.npy'))\n",
    "        return matrix\n",
    "\n",
    "    def get_binary_representation(self,lidar_input):\n",
    "        '''\n",
    "        Function that is used to get the binary representation of the Lidar input.\n",
    "        This function takes the Lidar input and projects it using the projection matrix.\n",
    "        It then binarizes the projected Lidar input and returns the binary representation.\n",
    "        the bias used is here is some types of non linear projections. It is set to zero here.\n",
    "        Args:\n",
    "            lidar_input (np.ndarray): Lidar input.\n",
    "        Returns:\n",
    "            np.ndarray: Binary representation of the Lidar input.\n",
    "        '''\n",
    "        self.normalized_lidar = normalize(lidar_input,axis=1)\n",
    "        # Do not normalize, just use the raw data\n",
    "        return self.binarize_vector(np.dot(lidar_input,self.projection_matrix) + self.bias)\n",
    "    \n",
    "\n",
    "    def get_state(self, binary):\n",
    "        return np.dot(binary[0], self.binary_powers)\n",
    "    \n",
    "\n",
    "    def select_action(self, state):\n",
    "        random_number = np.random.rand()\n",
    "        if random_number < self.action_threshold:\n",
    "            angle_index = np.random.randint(0, self.num_angles)\n",
    "            speed_index = np.random.randint(0, self.num_speeds)\n",
    "        else:\n",
    "            max_value = np.max(self.weights[state])\n",
    "            max_indices = np.argwhere(self.weights[state] == max_value)\n",
    "            angle_index, speed_index  = max_indices[np.random.randint(len(max_indices))]\n",
    "\n",
    "        self.action_threshold *= self.action_threshold_decay\n",
    "\n",
    "        return angle_index, speed_index\n",
    "\n",
    "    def select_action_inference(self, state):\n",
    "        max_indices = np.argwhere(self.weights[state] == np.max(self.weights[state]))\n",
    "        angle_index, speed_index  = max_indices[np.random.choice(np.arange(len(max_indices)))]\n",
    "        return angle_index, speed_index\n",
    "\n",
    "    def save_reward_time(self):\n",
    "        if not os.path.exists(os.path.join(self.save_path)):\n",
    "            os.mkdir(os.path.join(self.save_path))\n",
    "        \n",
    "        if self.reward_file is not None:\n",
    "            r = np.append(np.load(self.reward_file), self.episodic_rewards)\n",
    "            t = np.append(np.load(self.collision_file), self.collision_times)\n",
    "            np.save(os.path.join(self.save_path, f'rewards.npy'), np.array(r))\n",
    "            np.save(os.path.join(self.save_path, f'times.npy'), np.array(t))\n",
    "        else:\n",
    "            np.save(os.path.join(self.save_path, f'rewards.npy'), np.array(self.episodic_rewards))\n",
    "            np.save(os.path.join(self.save_path, f'times.npy'), np.array(self.collision_times))\n",
    "\n",
    "    def save_weights(self):\n",
    "        if not os.path.exists(os.path.join(self.save_path)):\n",
    "            os.mkdir(os.path.join(self.save_path))\n",
    "        np.save(os.path.join(self.save_path, f'{self.track_name[self.path_counter]}_{self.num_collisions + 1}.npy'), self.weights)\n",
    "        # print(f'File saved')\n",
    "\n",
    "    def inference(self):\n",
    "        try:\n",
    "            obs, step_reward, done, info = self.env.reset(np.array([[self.sx, self.sy, self.stheta[self.track_name[self.path_counter]]]]))\n",
    "            lidar = obs['scans'][0]\n",
    "            lidar_down_sampled = self.get_statistical_properties(lidar,n_sectors=self.n_sectors)\n",
    "            self.curr_state = self.get_state(self.get_binary_representation(lidar_down_sampled))\n",
    "            angle_index,speed_index = self.select_action_inference(self.curr_state)\n",
    "            while not done:\n",
    "                steering_angle,speed = self.angles[angle_index],self.speeds[speed_index]\n",
    "                obs, step_reward, done, info = self.env.step(np.array([[steering_angle, speed]]))\n",
    "                lidar = obs['scans'][0]\n",
    "                lidar_down_sampled = self.get_statistical_properties(lidar,n_sectors=self.n_sectors)\n",
    "                self.next_state = self.get_state(self.get_binary_representation(lidar_down_sampled))\n",
    "                angle_index,speed_index = self.select_action_inference(self.next_state)\n",
    "                self.curr_state = self.next_state\n",
    "                \n",
    "                self.env.render(mode='human')\n",
    "            raise Exception('Done')\n",
    "        except Exception as e:\n",
    "            print(f'Exception: {e}')\n",
    "            self.env.renderer.close()\n",
    "            self.env.close()\n",
    "            F110Env.renderer = None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hockenheim', 914),\n",
       " ('Mexico City', 860),\n",
       " ('Oschersleben', 739),\n",
       " ('Shanghai', 1090),\n",
       " ('BrandsHatch', 781),\n",
       " ('Monza', 1159),\n",
       " ('Catalunya', 931),\n",
       " ('SaoPaulo', 862),\n",
       " ('Sepang', 1108),\n",
       " ('Silverstone', 1178),\n",
       " ('Nuerburgring', 1029),\n",
       " ('YasMarina', 1110),\n",
       " ('Spa', 1401),\n",
       " ('Sochi', 1169),\n",
       " ('Montreal', 872),\n",
       " ('Austin', 1102),\n",
       " ('Melbourne', 1060),\n",
       " ('Budapest', 876),\n",
       " ('Spielberg', 864),\n",
       " ('Zandvoort', 864),\n",
       " ('Sakhir', 1082),\n",
       " ('MoscowRaceway', 813)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './f1tenth_racetracks'\n",
    "all_map_paths=[]\n",
    "map_centers = []\n",
    "maps = []\n",
    "track_lengths=[]\n",
    "for folder in os.listdir(path):\n",
    "    if folder not in ['README.md','.gitignore','convert.py','LICENSE','rename.py','.git']:\n",
    "        folder_name=folder\n",
    "        file_name=folder_name.replace(' ','')+'_map'\n",
    "        map_center = folder_name.replace(' ','')+'_centerline.csv'\n",
    "        track_lengths.append(len(pd.read_csv(f'{path}/{folder_name}/{map_center}')))\n",
    "        maps.append(folder_name)\n",
    "        all_map_paths.append(f'{path}/{folder_name}/{file_name}')\n",
    "        map_centers.append(f'{path}/{folder_name}/{map_center}')\n",
    "\n",
    "track_length_list = list(zip(maps,track_lengths))\n",
    "track_length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Maps: ['Spa', 'Hockenheim', 'Shanghai', 'Nuerburgring', 'Montreal', 'Austin', 'Mexico City']\n",
      "Test Maps: ['Oschersleben', 'BrandsHatch', 'Monza', 'Catalunya', 'SaoPaulo', 'Sepang', 'Silverstone', 'YasMarina', 'Sochi', 'Melbourne', 'Budapest', 'Spielberg', 'Zandvoort', 'Sakhir', 'MoscowRaceway']\n"
     ]
    }
   ],
   "source": [
    "# ['Austin','Mexico City','Spa','Hockenheim','Shanghai','Nuerburgring','Montreal'] - old train maps\n",
    "train_maps = ['Spa','Hockenheim','Shanghai','Nuerburgring','Montreal', 'Austin','Mexico City'] # 220 fov train maps\n",
    "test_maps = [i[0] for i in track_length_list if i[0] not in train_maps]\n",
    "print(f'Train Maps: {train_maps}')\n",
    "print(f'Test Maps: {test_maps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global num_agents,map_path,map_ext,sx,sy,stheta,indices\n",
    "num_agents = 1\n",
    "map_ext = '.png'\n",
    "sx = 0.\n",
    "sy = 0.\n",
    "stheta = {'Hockenheim': 2.02,\n",
    " 'Mexico City': -0.15,\n",
    " 'Oschersleben': 2.86,\n",
    " 'Shanghai': -2.93,\n",
    " 'BrandsHatch': 0.42,\n",
    " 'Monza': 1.47,\n",
    " 'Catalunya': -2.14,\n",
    " 'SaoPaulo': -1.31,\n",
    " 'Sepang': -3.06,\n",
    " 'Silverstone': 0.94,\n",
    " 'Nuerburgring': -2.38,\n",
    " 'YasMarina': 0.13,\n",
    " 'Spa': 2.13,\n",
    " 'Sochi': -2.14,\n",
    " 'Montreal': -1.35,\n",
    " 'Austin': -0.65,\n",
    " 'Melbourne': 2.37,\n",
    " 'Budapest': 2.45,\n",
    " 'Spielberg': -2.88,\n",
    " 'Zandvoort': 1.2,\n",
    " 'Sakhir': 1.53,\n",
    " 'MoscowRaceway': 1.46}\n",
    "\n",
    "# indices = [idx for idx,i in enumerate(maps) if i in test_maps]\n",
    "indices = [idx for idx,i in enumerate(maps) if i in train_maps]\n",
    "map_path_subset = [all_map_paths[i] for i in indices]\n",
    "map_centers_subset = [map_centers[i] for i in indices]\n",
    "map_names_subset = [maps[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praneeth/f1tenth_gym_ros/f1tenth_gym/gym/f110_gym/envs/base_classes.py:93: UserWarning: Chosen integrator is RK4. This is different from previous versions of the gym.\n",
      "  warnings.warn(f\"Chosen integrator is RK4. This is different from previous versions of the gym.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Weights\n",
      "BTSP Inference on Hockenheim\n",
      "Exception: Done\n"
     ]
    }
   ],
   "source": [
    "# inference_file = 'BTSP_Multiple_training/Austin_32000.npy'\n",
    "# reward_file='BTSP_Multiple_training/rewards.npy'\n",
    "# collision_file='BTSP_Multiple_training/times.npy'\n",
    "\n",
    "inference_file = 'BTSP_Multiple_training/Shanghai_10989.npy'\n",
    "reward_file='BTSP_Multiple_training/rewards.npy'\n",
    "collision_file='BTSP_Multiple_training/times.npy'\n",
    "\n",
    "map_index = 0\n",
    "\n",
    "map_path = [map_path_subset[map_index]]\n",
    "map_center = [map_centers_subset[map_index]]\n",
    "map_name = [map_names_subset[map_index]]\n",
    "\n",
    "save_path = 'Weights_BTSP/'\n",
    "\n",
    "simulator = F1Tenth_navigation(num_agents=num_agents,map_path=map_path,map_ext=map_ext,sx=sx,sy=sy,stheta=stheta,map_centers_file=map_centers,save_path=save_path,track_name=map_name,inference=inference_file,reward_file=reward_file,collision_file=collision_file)\n",
    "print(f'BTSP Inference on {map_names_subset[map_index]}')\n",
    "simulator.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simulator.weights[np.sum(simulator.weights,axis=(1,2))!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praneeth/f1tenth_gym_ros/f1tenth_gym/gym/f110_gym/envs/base_classes.py:93: UserWarning: Chosen integrator is RK4. This is different from previous versions of the gym.\n",
      "  warnings.warn(f\"Chosen integrator is RK4. This is different from previous versions of the gym.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Weights\n",
      "SARSA Inference on Hockenheim\n",
      "Exception: Done\n"
     ]
    }
   ],
   "source": [
    "# inference_file = 'SARSA_Multiple_training/Mexico City_33000.npy'\n",
    "# reward_file='SARSA_Multiple_training/rewards.npy'\n",
    "# collision_file='SARSA_Multiple_training/times.npy'\n",
    "\n",
    "inference_file = 'SARSA_Multiple_training/Montreal_10000.npy'\n",
    "reward_file='SARSA_Multiple_training/rewards.npy'\n",
    "collision_file='SARSA_Multiple_training/times.npy'\n",
    "\n",
    "\n",
    "# map_path_subset = [all_map_paths[i] for i in indices]\n",
    "# map_centers_subset = [map_centers[i] for i in indices]\n",
    "# map_names_subset = [map_names[i] for i in indices]\n",
    "\n",
    "# map_path = map_path_subset[map_index]\n",
    "# map_center = map_centers_subset[map_index]\n",
    "# map_name = map_names_subset[map_index]\n",
    "\n",
    "save_path = 'Weights_SARSA/'\n",
    "\n",
    "simulator = F1Tenth_navigation(num_agents=num_agents,map_path=map_path,map_ext=map_ext,sx=sx,sy=sy,stheta=stheta,map_centers_file=map_center,save_path=save_path,track_name=map_name,inference=inference_file,reward_file=reward_file,collision_file=collision_file)\n",
    "print(f'SARSA Inference on {map_names_subset[map_index]}')\n",
    "simulator.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simulator.weights[np.sum(simulator.weights,axis=(1,2))!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_file = 'Q_Multiple_training/MoscowRaceway_33000.npy'\n",
    "reward_file='Q_Multiple_training/rewards.npy'\n",
    "collision_file='Q_Multiple_training/times.npy'\n",
    "\n",
    "# map_path_subset = [all_map_paths[i] for i in indices]\n",
    "# map_centers_subset = [map_centers[i] for i in indices]\n",
    "# map_names_subset = [map_names[i] for i in indices]\n",
    "\n",
    "# map_path = map_path_subset[map_index]\n",
    "# map_center = map_centers_subset[map_index]\n",
    "# map_name = map_names_subset[map_index]\n",
    "\n",
    "save_path = 'Weights_Q/'\n",
    "\n",
    "simulator = F1Tenth_navigation(num_agents=num_agents,map_path=map_path_subset,map_ext=map_ext,sx=sx,sy=sy,stheta=stheta,map_centers_file=map_centers_subset,save_path=save_path,track_name=map_names_subset,inference=inference_file,reward_file=reward_file,collision_file=collision_file)\n",
    "print(f'SARSA Inference on {map_names_subset[map_index]}')\n",
    "simulator.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simulator.weights[np.sum(simulator.weights,axis=(1,2))!=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple track inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run the easy track with no curves\n",
    "inference_file = 'BTSP_Multiple_training/Mexico City_1765.npy'\n",
    "# inference_file = 'SARSA_Multiple_training/Nuerburgring_52000.npy'\n",
    "reward_file='BTSP_Multiple_training/rewards.npy'\n",
    "collision_file='BTSP_Multiple_training/times.npy'\n",
    "\n",
    "map_path_subset = ['/home/praneeth/shared_f1_tenth /IMS/IMS_map']\n",
    "map_centers_subset = ['/home/praneeth/shared_f1_tenth /IMS/IMS_centerline.csv']\n",
    "map_names_subset = ['IMS_map']\n",
    "\n",
    "map_index = 0\n",
    "\n",
    "# map_path = map_path_subset[map_index]\n",
    "# map_center = map_centers_subset[map_index]\n",
    "# map_name = map_names_subset[map_index]\n",
    "\n",
    "save_path = 'Weights_BTSP/'\n",
    "\n",
    "simulator = F1Tenth_navigation(num_agents=num_agents,map_path=map_path_subset,map_ext=map_ext,sx=sx,sy=sy,stheta=stheta,map_centers_file=map_centers_subset,save_path=save_path,track_name=map_names_subset,inference=inference_file,reward_file=reward_file,collision_file=collision_file)\n",
    "print(f'BTSP Inference on {map_name}')\n",
    "simulator.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btsp_times = []\n",
    "sarsa_times = []\n",
    "q_times = []\n",
    "\n",
    "inference_btsp = 'BTSP_Multiple_training/Austin_32000.npy'\n",
    "inference_sarsa = 'SARSA_Multiple_training/Mexico City_33000.npy'\n",
    "inference_q = 'Q_Multiple_training/MoscowRaceway_33000.npy'\n",
    "reward_file='BTSP_Multiple_training/rewards.npy'\n",
    "collision_file='BTSP_Multiple_training/times.npy'\n",
    "save_path = 'Weights_BTSP/'\n",
    "map_path_subset = [all_map_paths[i] for i in indices]\n",
    "map_centers_subset = [map_centers[i] for i in indices]\n",
    "map_names_subset = [map_names[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btsp_times_new = []\n",
    "sarsa_times_new = []\n",
    "q_times_new = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(map_centers_subset)):\n",
    "    map_path = map_path_subset[i]\n",
    "    map_center = map_centers_subset[i]\n",
    "    map_name = map_names_subset[i]\n",
    "\n",
    "    btsp_simulator = F1Tenth_navigation(num_agents=num_agents,map_path=map_path,map_ext=map_ext,sx=sx,sy=sy,stheta=stheta,map_centers_file=map_center,save_path=save_path,track_name=map_name,inference=inference_btsp,reward_file=reward_file,collision_file=collision_file)\n",
    "    start_time = time.time()\n",
    "    btsp_simulator.inference()\n",
    "    btsp_times_new.append(time.time() - start_time)\n",
    "    print(f'BTSP Inference on {map_name} took {btsp_times_new[-1]} seconds')\n",
    "\n",
    "    del btsp_simulator\n",
    "\n",
    "    sarsa_simulator = F1Tenth_navigation(num_agents=num_agents,map_path=map_path,map_ext=map_ext,sx=sx,sy=sy,stheta=stheta,map_centers_file=map_center,save_path=save_path,track_name=map_name,inference=inference_sarsa,reward_file=reward_file,collision_file=collision_file)\n",
    "    start_time = time.time()\n",
    "    sarsa_simulator.inference()\n",
    "    sarsa_times_new.append(time.time() - start_time)\n",
    "    print(f'SARSA Inference on {map_name} took {sarsa_times_new[-1]} seconds')\n",
    "\n",
    "    del sarsa_simulator\n",
    "\n",
    "    q_simulator = F1Tenth_navigation(num_agents=num_agents,map_path=map_path,map_ext=map_ext,sx=sx,sy=sy,stheta=stheta,map_centers_file=map_center,save_path=save_path,track_name=map_name,inference=inference_q,reward_file=reward_file,collision_file=collision_file)\n",
    "    start_time = time.time()\n",
    "    q_simulator.inference()\n",
    "    q_times_new.append(time.time() - start_time)\n",
    "    print(f'Q Inference on {map_name} took {q_times_new[-1]} seconds')\n",
    "\n",
    "    del q_simulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([btsp_times, sarsa_times, q_times], labels=['BTSP', 'SARSA', 'Q'])\n",
    "plt.title('Inference Times for Different Methods')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xlabel('Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([btsp_times_new, sarsa_times_new, q_times_new], labels=['HCL', 'SARSA', 'Q'])\n",
    "plt.title('Time to 1st collision (test tracks)')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('HCL_test_times.npy',btsp_times_new)    \n",
    "np.save('SARSA_test_times.npy',sarsa_times_new)\n",
    "np.save('Q_test_times.npy',q_times_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btsp_times = np.load('HCL_test_times.npy')\n",
    "sarsa_times = np.load('SARSA_test_times.npy')\n",
    "q_times = np.load('Q_test_times.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = plt.boxplot([btsp_times,sarsa_times,q_times],labels=['HCL', 'SARSA($\\lambda$)', 'Q-learning'],showfliers=False)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.title('Time to $1$ collision (Test)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
